{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Banknote dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankfile_path = 'Datasets/data_banknote_authentication.txt'\n",
    "\n",
    "# Initialize lists to hold features and labels\n",
    "features_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Open and read the file\n",
    "with open(bankfile_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line into components and convert them to the appropriate types\n",
    "        components = line.strip().split(',')\n",
    "        features = [float(num) for num in components[:-1]]  # Convert feature values to floats\n",
    "        label = int(components[-1])  # Convert the label to an integer\n",
    "        features_list.append(features)\n",
    "        labels_list.append(label)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "dataset = np.array(features_list)\n",
    "true_labels_for_points = np.array(labels_list)\n",
    "labels = np.unique(true_labels_for_points)\n",
    "\n",
    "# Create a features array with numerical labels as strings\n",
    "features = np.array([str(i) for i in range(dataset.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['0', '1', '2', '3'], dtype='<U1')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(features))\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(labels))\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Breast Cancer path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_path = 'Datasets/breast+cancer+coimbra/dataR2.csv'\n",
    "df = pd.read_csv(cancer_path)\n",
    "\n",
    "# Now, let's convert the DataFrame into the required numpy arrays\n",
    "# Extracting feature values and labels\n",
    "features_list = df.iloc[:, :-1].values\n",
    "\n",
    "# Extract the last column for labels\n",
    "labels_list = df.iloc[:, -1].values\n",
    "\n",
    "\n",
    "# Converting lists to NumPy arrays\n",
    "dataset = np.array(features_list)\n",
    "true_labels_for_points = np.array(labels_list)\n",
    "\n",
    "# Create a features array with numerical labels as strings\n",
    "features = np.array([str(i) for i in range(dataset.shape[1])])\n",
    "\n",
    "# Convert unique labels to numpy array\n",
    "labels = np.unique(true_labels_for_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cryotherapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "cryo_path = 'Datasets/Cryotherapy.xlsx'\n",
    "df = pd.read_excel(cryo_path)\n",
    "\n",
    "# Now, let's convert the DataFrame into the required numpy arrays\n",
    "# Extracting feature values and labels\n",
    "features_list = df.iloc[:, :-1].values\n",
    "\n",
    "# Extract the last column for labels\n",
    "labels_list = df.iloc[:, -1].values\n",
    "\n",
    "\n",
    "# Converting lists to NumPy arrays\n",
    "dataset = np.array(features_list)\n",
    "true_labels_for_points = np.array(labels_list)\n",
    "\n",
    "# Create a features array with numerical labels as strings\n",
    "features = np.array([str(i) for i in range(dataset.shape[1])])\n",
    "\n",
    "# Convert unique labels to numpy array\n",
    "labels = np.unique(true_labels_for_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Immunotherapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_path = 'Datasets/Immunotherapy.xlsx'\n",
    "df = pd.read_excel(im_path)\n",
    "\n",
    "# Now, let's convert the DataFrame into the required numpy arrays\n",
    "# Extracting feature values and labels\n",
    "features_list = df.iloc[:, :-1].values\n",
    "\n",
    "# Extract the last column for labels\n",
    "labels_list = df.iloc[:, -1].values\n",
    "\n",
    "\n",
    "# Converting lists to NumPy arrays\n",
    "dataset = np.array(features_list)\n",
    "true_labels_for_points = np.array(labels_list)\n",
    "\n",
    "# Create a features array with numerical labels as strings\n",
    "features = np.array([str(i) for i in range(dataset.shape[1])])\n",
    "\n",
    "# Convert unique labels to numpy array\n",
    "labels = np.unique(true_labels_for_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ionosphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ion_path = 'Datasets/ionosphere/ionosphere.data'\n",
    "\n",
    "# Initialize lists to hold features and labels\n",
    "features_list = []\n",
    "raw_labels_list = []\n",
    "\n",
    "# Open and read the file\n",
    "with open(ion_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line into components and convert them to the appropriate types\n",
    "        components = line.strip().split(',')\n",
    "        features = [float(num) for num in components[:-1]]  # Convert feature values to floats\n",
    "        raw_labels_list.append(components[-1])  # Keep the original label\n",
    "        features_list.append(features)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "dataset = np.array(features_list)\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit label encoder and return encoded labels\n",
    "true_labels_for_points = label_encoder.fit_transform(raw_labels_list)\n",
    "\n",
    "# Extract the unique labels as sorted array\n",
    "labels = np.sort(np.unique(true_labels_for_points))\n",
    "\n",
    "# Create a features array with numerical labels as strings\n",
    "features = np.array([str(i) for i in range(dataset.shape[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_path = 'Datasets/iris/iris.data'\n",
    "\n",
    "# Initialize lists to hold features and labels\n",
    "features_list = []\n",
    "raw_labels_list = []\n",
    "\n",
    "# Open and read the file\n",
    "with open(iris_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line into components and convert them to the appropriate types\n",
    "        components = line.strip().split(',')\n",
    "        features = [float(num) for num in components[:-1]]  # Convert feature values to floats\n",
    "        raw_labels_list.append(components[-1])  # Keep the original label\n",
    "        features_list.append(features)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "dataset = np.array(features_list)\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit label encoder and return encoded labels\n",
    "true_labels_for_points = label_encoder.fit_transform(raw_labels_list)\n",
    "\n",
    "# Extract the unique labels as sorted array\n",
    "labels = np.sort(np.unique(true_labels_for_points))\n",
    "\n",
    "# Create a features array with numerical labels as strings\n",
    "features = np.array([str(i) for i in range(dataset.shape[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_path = 'Datasets/Data_User_Modeling_Dataset_Hamdi_Tolga_KAHRAMAN.xls'\n",
    "df = pd.read_excel(user_path)\n",
    "\n",
    "# Now, let's convert the DataFrame into the required numpy arrays\n",
    "# Extracting feature values and labels\n",
    "features_list = df.iloc[:, :-1].values\n",
    "\n",
    "# Extract the last column for labels\n",
    "labels_list = df.iloc[:, -1].values\n",
    "\n",
    "\n",
    "# Converting lists to NumPy arrays\n",
    "dataset = np.array(features_list)\n",
    "true_labels_for_points = np.array(labels_list)\n",
    "true_labels_for_points = label_encoder.fit_transform(true_labels_for_points)\n",
    "\n",
    "# Extract the unique labels as sorted array\n",
    "labels = np.sort(np.unique(true_labels_for_points))\n",
    "\n",
    "# Create a features array with numerical labels as strings\n",
    "features = np.array([str(i) for i in range(dataset.shape[1])])\n",
    "\n",
    "# Convert unique labels to numpy array\n",
    "labels = np.unique(true_labels_for_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vertebratla column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_path = 'Datasets/vertebral+column/verbex.data'\n",
    "\n",
    "# Initialize lists to hold features and labels\n",
    "features_list = []\n",
    "raw_labels_list = []\n",
    "\n",
    "# Open and read the file\n",
    "with open(verba_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line into components and convert them to the appropriate types\n",
    "        components = line.strip().split(',')\n",
    "        features = [float(num) for num in components[:-1]]  # Convert feature values to floats\n",
    "        raw_labels_list.append(components[-1])  # Keep the original label\n",
    "        features_list.append(features)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "dataset = np.array(features_list)\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit label encoder and return encoded labels\n",
    "true_labels_for_points = label_encoder.fit_transform(raw_labels_list)\n",
    "\n",
    "# Extract the unique labels as sorted array\n",
    "labels = np.sort(np.unique(true_labels_for_points))\n",
    "\n",
    "# Create a features array with numerical labels as strings\n",
    "features = np.array([str(i) for i in range(dataset.shape[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_path = 'Datasets/wine/wine.data'\n",
    "\n",
    "# THIS ONE IS A BIT DIFFERENT LABEL IS ON FIRST FEATURW \n",
    "# Initialize lists to hold features and raw labels\n",
    "features_list = []\n",
    "raw_labels_list = []\n",
    "\n",
    "# Open and read the file\n",
    "with open(wine_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line into components and convert them to the appropriate types\n",
    "        components = line.strip().split(',')\n",
    "        raw_labels_list.append(components[0])  # The first element is the label\n",
    "        features = [float(num) for num in components[1:]]  # The rest are features\n",
    "        features_list.append(features)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "dataset = np.array(features_list)\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit label encoder and return encoded labels\n",
    "true_labels_for_points = label_encoder.fit_transform(raw_labels_list)\n",
    "\n",
    "# Extract the unique labels as sorted array\n",
    "labels = np.sort(np.unique(true_labels_for_points))\n",
    "\n",
    "# Create a features array with numerical labels as strings\n",
    "features = np.array([str(i) for i in range(dataset.shape[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monk-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming 'wine_path' is the path to the text file containing the dataset\n",
    "monk_path = 'Datasets/monk+s+problems/monks-2.train' # Update this to the path of your data file\n",
    "\n",
    "# Initialize lists to hold features and labels\n",
    "features_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Open and read the file\n",
    "with open(monk_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line into components based on whitespace\n",
    "        components = line.strip().split()\n",
    "        # The second-to-last element is the label, and the rest (excluding the last element) are features\n",
    "        labels_list.append(components[0])  # Second-to-last element as label\n",
    "        features = [float(num) for num in components[1:-1]]  # Exclude last two elements\n",
    "        features_list.append(features)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "dataset = np.array(features_list)\n",
    "true_labels_for_points = LabelEncoder().fit_transform(labels_list)\n",
    "\n",
    "# The unique labels\n",
    "labels = np.unique(true_labels_for_points)\n",
    "\n",
    "# Create a features array with numerical labels as strings\n",
    "features = np.array([str(i) for i in range(dataset.shape[1])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree Building now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found at depth: 6\n"
     ]
    }
   ],
   "source": [
    "from min_height_tree_module import *\n",
    "min_depth_tree, min_depth_literals, min_depth,solution = find_min_depth_tree(features, labels, true_labels_for_points, dataset)\n",
    "#print(\"Minimum Depth Tree Structure:\")\n",
    "#for node in min_depth_tree:\n",
    "#    print(node)\n",
    "print(f\"Found at depth: {min_depth}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
