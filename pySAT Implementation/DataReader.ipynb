{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Banknote dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankfile_path = 'Datasets/data_banknote_authentication.txt'\n",
    "\n",
    "# Initialize lists to hold features and labels\n",
    "features_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Open and read the file\n",
    "with open(bankfile_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line into components and convert them to the appropriate types\n",
    "        components = line.strip().split(',')\n",
    "        features = [float(num) for num in components[:-1]]  # Convert feature values to floats\n",
    "        label = int(components[-1])  # Convert the label to an integer\n",
    "        features_list.append(features)\n",
    "        labels_list.append(label)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "dataset = np.array(features_list)\n",
    "true_labels_for_points = np.array(labels_list)\n",
    "labels = np.unique(true_labels_for_points)\n",
    "\n",
    "# Create a features array with numerical labels as strings\n",
    "features = np.array([str(i) for i in range(dataset.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(features))\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(labels))\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Breast Cancer path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_path = 'Datasets/breast+cancer+coimbra/dataR2.csv'\n",
    "df = pd.read_csv(cancer_path)\n",
    "\n",
    "# Now, let's convert the DataFrame into the required numpy arrays\n",
    "# Extracting feature values and labels\n",
    "features_list = df.iloc[:, :-1].values\n",
    "\n",
    "# Extract the last column for labels\n",
    "labels_list = df.iloc[:, -1].values\n",
    "\n",
    "\n",
    "# Converting lists to NumPy arrays\n",
    "dataset = np.array(features_list)\n",
    "true_labels_for_points = np.array(labels_list)\n",
    "\n",
    "# Create a features array with numerical labels as strings\n",
    "features = np.array([str(i) for i in range(dataset.shape[1])])\n",
    "\n",
    "# Convert unique labels to numpy array\n",
    "labels = np.unique(true_labels_for_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cryotherapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cryo_path = 'Datasets/Cryotherapy.xlsx'\n",
    "df = pd.read_excel(cryo_path)\n",
    "\n",
    "# Now, let's convert the DataFrame into the required numpy arrays\n",
    "# Extracting feature values and labels\n",
    "features_list = df.iloc[:, :-1].values\n",
    "\n",
    "# Extract the last column for labels\n",
    "labels_list = df.iloc[:, -1].values\n",
    "\n",
    "\n",
    "# Converting lists to NumPy arrays\n",
    "dataset = np.array(features_list)\n",
    "true_labels_for_points = np.array(labels_list)\n",
    "\n",
    "# Create a features array with numerical labels as strings\n",
    "features = np.array([str(i) for i in range(dataset.shape[1])])\n",
    "\n",
    "# Convert unique labels to numpy array\n",
    "labels = np.unique(true_labels_for_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Immunotherapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_path = 'Datasets/Immunotherapy.xlsx'\n",
    "df = pd.read_excel(im_path)\n",
    "\n",
    "# Now, let's convert the DataFrame into the required numpy arrays\n",
    "# Extracting feature values and labels\n",
    "features_list = df.iloc[:, :-1].values\n",
    "\n",
    "# Extract the last column for labels\n",
    "labels_list = df.iloc[:, -1].values\n",
    "\n",
    "\n",
    "# Converting lists to NumPy arrays\n",
    "dataset = np.array(features_list)\n",
    "true_labels_for_points = np.array(labels_list)\n",
    "\n",
    "# Create a features array with numerical labels as strings\n",
    "features = np.array([str(i) for i in range(dataset.shape[1])])\n",
    "\n",
    "# Convert unique labels to numpy array\n",
    "labels = np.unique(true_labels_for_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ionosphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ion_path = 'Datasets/ionosphere/ionosphere.data'\n",
    "\n",
    "# Initialize lists to hold features and labels\n",
    "features_list = []\n",
    "raw_labels_list = []\n",
    "\n",
    "# Open and read the file\n",
    "with open(ion_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line into components and convert them to the appropriate types\n",
    "        components = line.strip().split(',')\n",
    "        features = [float(num) for num in components[:-1]]  # Convert feature values to floats\n",
    "        raw_labels_list.append(components[-1])  # Keep the original label\n",
    "        features_list.append(features)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "dataset = np.array(features_list)\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit label encoder and return encoded labels\n",
    "true_labels_for_points = label_encoder.fit_transform(raw_labels_list)\n",
    "\n",
    "# Extract the unique labels as sorted array\n",
    "labels = np.sort(np.unique(true_labels_for_points))\n",
    "\n",
    "# Create a features array with numerical labels as strings\n",
    "features = np.array([str(i) for i in range(dataset.shape[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_path = 'Datasets/iris/iris.data'\n",
    "\n",
    "# Initialize lists to hold features and labels\n",
    "features_list = []\n",
    "raw_labels_list = []\n",
    "\n",
    "# Open and read the file\n",
    "with open(iris_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line into components and convert them to the appropriate types\n",
    "        components = line.strip().split(',')\n",
    "        features = [float(num) for num in components[:-1]]  # Convert feature values to floats\n",
    "        raw_labels_list.append(components[-1])  # Keep the original label\n",
    "        features_list.append(features)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "dataset = np.array(features_list)\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit label encoder and return encoded labels\n",
    "true_labels_for_points = label_encoder.fit_transform(raw_labels_list)\n",
    "\n",
    "# Extract the unique labels as sorted array\n",
    "labels = np.sort(np.unique(true_labels_for_points))\n",
    "\n",
    "# Create a features array with numerical labels as strings\n",
    "features = np.array([str(i) for i in range(dataset.shape[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_path = 'Datasets/Data_User_Modeling_Dataset_Hamdi_Tolga_KAHRAMAN.xls'\n",
    "df = pd.read_excel(user_path)\n",
    "\n",
    "# Now, let's convert the DataFrame into the required numpy arrays\n",
    "# Extracting feature values and labels\n",
    "features_list = df.iloc[:, :-1].values\n",
    "\n",
    "# Extract the last column for labels\n",
    "labels_list = df.iloc[:, -1].values\n",
    "\n",
    "\n",
    "# Converting lists to NumPy arrays\n",
    "dataset = np.array(features_list)\n",
    "true_labels_for_points = np.array(labels_list)\n",
    "true_labels_for_points = label_encoder.fit_transform(true_labels_for_points)\n",
    "\n",
    "# Extract the unique labels as sorted array\n",
    "labels = np.sort(np.unique(true_labels_for_points))\n",
    "\n",
    "# Create a features array with numerical labels as strings\n",
    "features = np.array([str(i) for i in range(dataset.shape[1])])\n",
    "\n",
    "# Convert unique labels to numpy array\n",
    "labels = np.unique(true_labels_for_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vertebratla column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_path = 'Datasets/vertebral+column/verbex.data'\n",
    "\n",
    "# Initialize lists to hold features and labels\n",
    "features_list = []\n",
    "raw_labels_list = []\n",
    "\n",
    "# Open and read the file\n",
    "with open(verba_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line into components and convert them to the appropriate types\n",
    "        components = line.strip().split(',')\n",
    "        features = [float(num) for num in components[:-1]]  # Convert feature values to floats\n",
    "        raw_labels_list.append(components[-1])  # Keep the original label\n",
    "        features_list.append(features)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "dataset = np.array(features_list)\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit label encoder and return encoded labels\n",
    "true_labels_for_points = label_encoder.fit_transform(raw_labels_list)\n",
    "\n",
    "# Extract the unique labels as sorted array\n",
    "labels = np.sort(np.unique(true_labels_for_points))\n",
    "\n",
    "# Create a features array with numerical labels as strings\n",
    "features = np.array([str(i) for i in range(dataset.shape[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_path = 'Datasets/wine/wine.data'\n",
    "\n",
    "# THIS ONE IS A BIT DIFFERENT LABEL IS ON FIRST FEATURW \n",
    "# Initialize lists to hold features and raw labels\n",
    "features_list = []\n",
    "raw_labels_list = []\n",
    "\n",
    "# Open and read the file\n",
    "with open(wine_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line into components and convert them to the appropriate types\n",
    "        components = line.strip().split(',')\n",
    "        raw_labels_list.append(components[0])  # The first element is the label\n",
    "        features = [float(num) for num in components[1:]]  # The rest are features\n",
    "        features_list.append(features)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "dataset = np.array(features_list)\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit label encoder and return encoded labels\n",
    "true_labels_for_points = label_encoder.fit_transform(raw_labels_list)\n",
    "\n",
    "# Extract the unique labels as sorted array\n",
    "labels = np.sort(np.unique(true_labels_for_points))\n",
    "\n",
    "# Create a features array with numerical labels as strings\n",
    "features = np.array([str(i) for i in range(dataset.shape[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monk-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming 'wine_path' is the path to the text file containing the dataset\n",
    "monk_path = 'Datasets/monk+s+problems/monks-2.train' # Update this to the path of your data file\n",
    "\n",
    "# Initialize lists to hold features and labels\n",
    "features_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Open and read the file\n",
    "with open(monk_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line into components based on whitespace\n",
    "        components = line.strip().split()\n",
    "        # The second-to-last element is the label, and the rest (excluding the last element) are features\n",
    "        labels_list.append(components[0])  # Second-to-last element as label\n",
    "        features = [float(num) for num in components[1:-1]]  # Exclude last two elements\n",
    "        features_list.append(features)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "dataset = np.array(features_list)\n",
    "true_labels_for_points = LabelEncoder().fit_transform(labels_list)\n",
    "\n",
    "# The unique labels\n",
    "labels = np.unique(true_labels_for_points)\n",
    "\n",
    "# Create a features array with numerical labels as strings\n",
    "features = np.array([str(i) for i in range(dataset.shape[1])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree Building now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min height Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "file_path_to_test = 'Datasets/wine/wine.data'\n",
    "X = TreeDataLoaderBinaryNumerical(file_path=file_path_to_test, delimiter=',', label_position= -1)\n",
    "print(len(X.dataset))\n",
    "print(len(X.features))\n",
    "print(X.features)\n",
    "print(X.labels)\n",
    "print(len(X.true_labels_for_points))\n",
    "features = X.features\n",
    "labels = X.labels\n",
    "true_labels_for_points = X.true_labels_for_points\n",
    "dataset = X.dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_problems.min_height_tree_module import *\n",
    "min_depth_tree, min_depth_literals, min_depth,solution = find_min_depth_tree(features, labels, true_labels_for_points, dataset)\n",
    "#print(\"Minimum Depth Tree Structure:\")\n",
    "#for node in min_depth_tree:\n",
    "#    print(node)\n",
    "print(f\"Found at depth: {min_depth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAX SAT PROBLEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from classification_problems.fixed_height_tree_module import * \n",
    "file_path_to_test = 'Datasets/monk+s+problems/monks-2.train'\n",
    "X = TreeDataLoaderBinaryNumerical(file_path=file_path_to_test, delimiter=' ', label_position= 0, custom_exclude=[-1])\n",
    "print(len(X.dataset))\n",
    "print(len(X.features))\n",
    "print(X.features)\n",
    "print(X.labels)\n",
    "print(len(X.true_labels_for_points))\n",
    "\n",
    "features = X.features\n",
    "labels = X.labels\n",
    "true_labels_for_points = X.true_labels_for_points\n",
    "dataset = X.dataset\n",
    "depth = 2\n",
    "print('depth: ', depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_with_thresholds, literals, depth, solution, cost =find_fixed_depth_tree(features, labels, true_labels_for_points, dataset,depth)\n",
    "print(\"The cost of the solution is: \", cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical Testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min Height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit Approval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_file(path):\n",
    "    \"\"\"\n",
    "    Parses the data from the file at the given path.\n",
    "    \n",
    "    Args:\n",
    "    - path (str): The file path to the dataset.\n",
    "    \n",
    "    Returns:\n",
    "    - proccesed raw data\n",
    "    \"\"\"\n",
    "    # Define the indices for the different types of features\n",
    "    len_row = 16\n",
    "    # Containers for the data\n",
    "    raw_data = []\n",
    "    len_lists = set()\n",
    "    attribute_set = set()\n",
    "    # Read the file\n",
    "    with open(path, 'r') as file:\n",
    "        count = 0\n",
    "        for line in file:\n",
    "            # Split the row into columns\n",
    "            columns = line.strip().split(',')\n",
    "            length = len(columns)\n",
    "            len_lists.add(length)\n",
    "            for item in columns:\n",
    "                attribute_set.add(item)\n",
    "            # Skip rows with missing values\n",
    "            if '?' in columns:\n",
    "                continue\n",
    "            else:\n",
    "                raw_data.append(columns)\n",
    "    return raw_data\n",
    "    \n",
    "data_array = parse_data_file('Datasets/credit+approval/crx.data')\n",
    "data_array[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_data = []\n",
    "true_labels_for_points = []  # To store true labels\n",
    "for values in data_array:\n",
    "    # Convert class labels from {+,-} to {1,0}\n",
    "    label = '1' if values[-1] == '+' else '0'\n",
    "    true_labels_for_points.append(int(label))\n",
    "    # remove leading zeros from numbers on idnex 13\n",
    "    # Remove the label from the features\n",
    "    del values[-1]\n",
    "    # Add the processed row to the structured data\n",
    "    structured_data.append(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array([str(i) for i in range(len(structured_data[0]))])\n",
    "dataset = np.array(structured_data)\n",
    "labels = np.array(sorted(list(set(true_labels_for_points))))\n",
    "features_numerical = np.array(['1', '2', '7', '10', '13','14'])  # Given numerical features\n",
    "features_categorical = np.array(['0','3', '4', '5', '6', '8','9','11','12'])   # Given categorical features\n",
    "true_labels_for_points = np.array(true_labels_for_points)\n",
    "print(\"Fetaures: \", features,features.shape)\n",
    "print(\"Dataset: \", dataset, dataset.shape)\n",
    "print(\"X_0: \", dataset[0])\n",
    "print(\"FC: \", features_categorical,features_categorical.shape)\n",
    "print(\"FN: \", features_numerical,features_numerical.shape)\n",
    "print(\"Labels: \",true_labels_for_points,true_labels_for_points.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_problems.min_height_tree_categorical_module import find_min_depth_tree_categorical\n",
    "find_min_depth_tree_categorical(features, features_categorical, features_numerical, labels, true_labels_for_points, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_problems.fixed_height_tree_categorical_module import find_fixed_depth_tree_categorical\n",
    "tree_with_thresholds, literals, depth, solution, cost = find_fixed_depth_tree_categorical(features, features_categorical, features_numerical, labels, true_labels_for_points, dataset, 2)\n",
    "print(\"The cost of the solution is: \", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_with_thresholds, literals, depth, solution, cost = find_fixed_depth_tree_categorical(features, features_categorical, features_numerical, labels, true_labels_for_points, dataset, 3)\n",
    "print(\"The cost of the solution is: \", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_with_thresholds, literals, depth, solution, cost = find_fixed_depth_tree_categorical(features, features_categorical, features_numerical, labels, true_labels_for_points, dataset, 4)\n",
    "print(\"The cost of the solution is: \", cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Promoter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def process_line(line):\n",
    "    parts = line.strip().split(',')\n",
    "    label = 1 if parts[0] == '+' else 0\n",
    "    features = list(parts[2].strip())\n",
    "    return label, features\n",
    "\n",
    "def load_data(file_path):\n",
    "    labels = []\n",
    "    dataset = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            label, features = process_line(line)\n",
    "            labels.append(label)\n",
    "            dataset.append(features)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    labels = np.array(labels)\n",
    "    dataset = np.array(dataset)\n",
    "\n",
    "    # Assuming all sequences are of the same length to determine the number of features\n",
    "    feature_names = np.array([str(i) for i in range(len(dataset[0]))])\n",
    "\n",
    "    return labels, dataset, feature_names\n",
    "\n",
    "# Path to your data file\n",
    "file_path = 'Datasets/molecular+biology+promoter+gene+sequences/promoters.data'\n",
    "\n",
    "# Load the data\n",
    "true_labels_for_points, dataset, features = load_data(file_path)\n",
    "\n",
    "# Since in this case, all features are categorical\n",
    "features_categorical = features\n",
    "labels = np.array([1, 0])\n",
    "features_numerical = np.array([])\n",
    "\n",
    "# Output for verification\n",
    "print(\"Features:\", features, len(features))\n",
    "print('dataset: ', dataset, len(dataset))\n",
    "print(\"Labels for points:\", true_labels_for_points, len(true_labels_for_points))\n",
    "print(\"Labels:\", labels, len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_problems.min_height_tree_categorical_module import find_min_depth_tree_categorical\n",
    "min_depth_tree, min_depth_literals, min_depth,solution = find_min_depth_tree_categorical(features, features_categorical, features_numerical, labels, true_labels_for_points, dataset)\n",
    "print(\"Minimum Depth Tree Structure: \", min_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_problems.fixed_height_tree_categorical_module import find_fixed_depth_tree_categorical\n",
    "tree_with_thresholds, literals, depth, solution, cost = find_fixed_depth_tree_categorical(features, features_categorical, features_numerical, labels, true_labels_for_points, dataset, 2)\n",
    "print(\"The cost of the solution is: \", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_with_thresholds, literals, depth, solution, cost = find_fixed_depth_tree_categorical(features, features_categorical, features_numerical, labels, true_labels_for_points, dataset, 3)\n",
    "print(\"The cost of the solution is: \", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_with_thresholds, literals, depth, solution, cost = find_fixed_depth_tree_categorical(features, features_categorical, features_numerical, labels, true_labels_for_points, dataset, 4)\n",
    "print(\"The cost of the solution is: \", cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Protease Cleavage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def process_line(line):\n",
    "    parts = line.strip().split(',')\n",
    "    sequence = list(parts[0].strip())\n",
    "    label = 0 if parts[1] == '-1' else 1\n",
    "    return label, sequence\n",
    "\n",
    "def load_data(file_path):\n",
    "    labels = []\n",
    "    dataset = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            label, sequence = process_line(line)\n",
    "            labels.append(label)\n",
    "            dataset.append(sequence)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    labels = np.array(labels)\n",
    "    dataset = np.array(dataset)\n",
    "\n",
    "    # Generate feature names based on the length of the first sequence\n",
    "    feature_names = np.array([str(i) for i in range(len(dataset[0]))])\n",
    "\n",
    "    return labels, dataset, feature_names\n",
    "\n",
    "# Path to your data file\n",
    "file_path = 'Datasets/hiv+1+protease+cleavage/746Data.txt'\n",
    "\n",
    "# Load the data\n",
    "true_labels_for_points, dataset, features = load_data(file_path)\n",
    "\n",
    "# Since in this case, all features are categorical\n",
    "features_categorical = features\n",
    "labels = np.array([1, 0])\n",
    "features_numerical = np.array([])\n",
    "\n",
    "# Output for verification\n",
    "print(\"Features:\", features, len(features))\n",
    "print('dataset: ', dataset, len(dataset))\n",
    "print(\"Labels for points:\", true_labels_for_points, len(true_labels_for_points))\n",
    "print(\"Labels:\", labels, len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_problems.min_height_tree_categorical_module import find_min_depth_tree_categorical\n",
    "min_depth_tree, min_depth_literals, min_depth,solution = find_min_depth_tree_categorical(features, features_categorical, features_numerical, labels, true_labels_for_points, dataset)\n",
    "print(\"Minimum Depth Tree Structure: \", min_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_problems.fixed_height_tree_categorical_module import find_fixed_depth_tree_categorical\n",
    "tree_with_thresholds, literals, depth, solution, cost = find_fixed_depth_tree_categorical(features, features_categorical, features_numerical, labels, true_labels_for_points, dataset, 2)\n",
    "print(\"The cost of the solution is: \", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_with_thresholds, literals, depth, solution, cost = find_fixed_depth_tree_categorical(features, features_categorical, features_numerical, labels, true_labels_for_points, dataset, 3)\n",
    "print(\"The cost of the solution is: \", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_with_thresholds, literals, depth, solution, cost = find_fixed_depth_tree_categorical(features, features_categorical, features_numerical, labels, true_labels_for_points, dataset, 4)\n",
    "print(\"The cost of the solution is: \", cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Protease /4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_dataset = dataset[3::4]\n",
    "selected_labels = true_labels_for_points[3::4]\n",
    "#selected_dataset = selected_dataset[1:]\n",
    "#selected_labels = selected_labels[1:]\n",
    "\n",
    "\n",
    "print('dataset: ', selected_dataset, len(selected_dataset))\n",
    "print(\"Labels for points:\", selected_labels, len(selected_labels))\n",
    "print(\"Features:\", features, len(features))\n",
    "print(\"Labels:\", labels, len(labels))\n",
    "print(\"Features C:\",features_categorical,len(features_categorical))\n",
    "print(\"Features N:\",features_numerical,len(features_numerical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_problems.min_height_tree_categorical_module import find_min_depth_tree_categorical\n",
    "min_depth_tree, min_depth_literals, min_depth,solution = find_min_depth_tree_categorical(features, features_categorical, \n",
    "    features_numerical, labels, selected_labels, selected_dataset)\n",
    "print(\"Minimum Depth Tree Structure: \", min_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_problems.fixed_height_tree_categorical_module import find_fixed_depth_tree_categorical\n",
    "tree_with_thresholds, literals, depth, solution, cost = find_fixed_depth_tree_categorical(features, features_categorical, \n",
    "    features_numerical, labels, selected_labels, selected_dataset, 2)\n",
    "print(\"The cost of the solution is: \", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_with_thresholds, literals, depth, solution, cost = find_fixed_depth_tree_categorical(features, features_categorical, \n",
    "    features_numerical, labels, selected_labels, selected_dataset, 3)\n",
    "print(\"The cost of the solution is: \", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_with_thresholds, literals, depth, solution, cost = find_fixed_depth_tree_categorical(features, features_categorical, \n",
    "    features_numerical, labels, selected_labels, selected_dataset, 4)\n",
    "print(\"The cost of the solution is: \", cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soybean Large "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the path to your data file\n",
    "file_path = 'Datasets/soybean+large/soybean-large.data'\n",
    "\n",
    "# Initialize lists to hold the dataset and labels\n",
    "dataset = []\n",
    "labels = []\n",
    "\n",
    "# Dictionary to map textual labels to numeric labels\n",
    "label_mapping = {}\n",
    "label_counter = 0\n",
    "\n",
    "# Read the file\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Skip if there's a missing value\n",
    "        if '?' in line:\n",
    "            continue\n",
    "        \n",
    "        # Split the line into parts and extract the label and features\n",
    "        parts = line.strip().split(',')\n",
    "        label = parts[0]\n",
    "        features = parts[1:]\n",
    "        \n",
    "        # If the label is new, add it to the label mapping\n",
    "        if label not in label_mapping:\n",
    "            label_mapping[label] = label_counter\n",
    "            label_counter += 1\n",
    "        \n",
    "        # Add the numeric label and features to their respective lists\n",
    "        labels.append(label_mapping[label])\n",
    "        dataset.append(features)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "dataset = np.array(dataset, dtype=str)  # Assuming features are numeric\n",
    "true_labels_for_points = np.array(labels, dtype=int)\n",
    "features = [str(i) for i in range(dataset.shape[1])]\n",
    "features = np.array(features)\n",
    "labels = np.unique(true_labels_for_points)\n",
    "\n",
    "# features_numerical and categorical \n",
    "features_categorical = features\n",
    "features_numerical = np.array([])\n",
    "\n",
    "# Output for verification\n",
    "print(\"Label mapping:\", label_mapping)\n",
    "print(\"Dataset: \", dataset, dataset.shape)\n",
    "print(\"Labels for points shape: \", true_labels_for_points, true_labels_for_points.shape)\n",
    "print(\"Features: \", features, features.shape)\n",
    "print(\"Unique class labels: \", labels, labels.shape)\n",
    "print(\"Features C: \", features_categorical, features_categorical.shape)\n",
    "print(\"Features N: \", features_numerical, features_numerical.shape)\n",
    "print('Data Point 0: ', dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_problems.min_height_tree_categorical_module import find_min_depth_tree_categorical\n",
    "min_depth_tree, min_depth_literals, min_depth,solution = find_min_depth_tree_categorical(features, features_categorical, features_numerical, labels, true_labels_for_points, dataset)\n",
    "print(\"Minimum Depth Tree Structure: \", min_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_problems.fixed_height_tree_categorical_module import find_fixed_depth_tree_categorical\n",
    "tree_with_thresholds, literals, depth, solution, cost = find_fixed_depth_tree_categorical(features, features_categorical, features_numerical, labels, true_labels_for_points, dataset, 2)\n",
    "print(\"The cost of the solution is: \", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_problems.fixed_height_tree_categorical_module import find_fixed_depth_tree_categorical\n",
    "tree_with_thresholds, literals, depth, solution, cost = find_fixed_depth_tree_categorical(features, features_categorical, features_numerical, labels, true_labels_for_points, dataset, 3)\n",
    "print(\"The cost of the solution is: \", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_problems.fixed_height_tree_categorical_module import find_fixed_depth_tree_categorical\n",
    "tree_with_thresholds, literals, depth, solution, cost = find_fixed_depth_tree_categorical(features, features_categorical, features_numerical, labels, true_labels_for_points, dataset, 4)\n",
    "print(\"The cost of the solution is: \", cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Car Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Define the path to your data file\n",
    "file_path = 'Datasets/car+evaluation/car.data'\n",
    "\n",
    "# Initialize lists to hold the dataset and labels\n",
    "dataset = []\n",
    "labels = []\n",
    "\n",
    "# Dictionary to map textual labels to numeric labels with the specified encoding\n",
    "label_mapping = {'unacc': 0, 'acc': 1, 'good': 1, 'vgood': 1}\n",
    "\n",
    "# Read the file\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Skip if there's a missing value\n",
    "        if '?' in line:\n",
    "            continue\n",
    "        \n",
    "        # Split the line into parts and extract the features and label\n",
    "        parts = line.strip().split(',')\n",
    "        features = parts[:-1]  # All but the last item\n",
    "        label = parts[-1]  # The last item\n",
    "        \n",
    "        # Add the numeric label and features to their respective lists\n",
    "        labels.append(label_mapping[label])\n",
    "        dataset.append(features)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "dataset = np.array(dataset, dtype=str)\n",
    "true_labels_for_points = np.array(labels, dtype=int)\n",
    "\n",
    "# Generate feature names based on the dataset shape\n",
    "features = [str(i) for i in range(dataset.shape[1])]\n",
    "features = np.array(features)\n",
    "features_categorical = np.array(features)\n",
    "features_numerical = np.array([])\n",
    "labels = np.unique(true_labels_for_points)\n",
    "\n",
    "# Output for verification\n",
    "print(\"Label mapping:\", label_mapping)\n",
    "print(\"Dataset: \", dataset, dataset.shape)\n",
    "print(\"Labels for points shape: \", true_labels_for_points, true_labels_for_points.shape)\n",
    "print(\"Features: \", features, features.shape)\n",
    "print(\"Unique class labels: \", labels, labels.shape)\n",
    "print(\"Features C: \", features_categorical, features_categorical.shape)\n",
    "print(\"Features N: \", features_numerical, features_numerical.shape)\n",
    "print('Data Point 0: ', dataset[0], true_labels_for_points[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_problems.min_height_tree_categorical_module import find_min_depth_tree_categorical\n",
    "min_depth_tree, min_depth_literals, min_depth,solution = find_min_depth_tree_categorical(features, features_categorical, features_numerical, labels, true_labels_for_points, dataset)\n",
    "print(\"Minimum Depth Tree Structure: \", min_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_problems.fixed_height_tree_categorical_module import find_fixed_depth_tree_categorical\n",
    "tree_with_thresholds, literals, depth, solution, cost = find_fixed_depth_tree_categorical(features, features_categorical, features_numerical, labels, true_labels_for_points, dataset, 2)\n",
    "print(\"The cost of the solution is: \", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_with_thresholds, literals, depth, solution, cost = find_fixed_depth_tree_categorical(features, features_categorical, features_numerical, labels, true_labels_for_points, dataset, 3)\n",
    "print(\"The cost of the solution is: \", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_with_thresholds, literals, depth, solution, cost = find_fixed_depth_tree_categorical(features, features_categorical, features_numerical, labels, true_labels_for_points, dataset, 4)\n",
    "print(\"The cost of the solution is: \", cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical Data Loader test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "# Example usage:\n",
    "file_path = 'Datasets/hiv+1+protease+cleavage/746Data.txt'\n",
    "label_index = 1\n",
    "categorical_feature_index = 0  # The features are a string at the third element\n",
    "numerical_indices = None\n",
    "\n",
    "data_loader = TreeDataLoaderWithCategorical(\n",
    "    file_path= file_path,\n",
    "    label_index= label_index,\n",
    "    numerical_indices= numerical_indices,\n",
    "    categorical_feature_index=categorical_feature_index\n",
    ")\n",
    "\n",
    "# Accessing the processed data\n",
    "print(\"Features:\", data_loader.features)\n",
    "print(\"Categorical Features:\", data_loader.features_categorical)\n",
    "print(\"Numerical Features:\", data_loader.features_numerical)\n",
    "print(\"Labels:\", data_loader.labels)\n",
    "print(\"True Labels for Points:\", data_loader.true_labels_for_points)\n",
    "print(\"Dataset:\\n\", data_loader.dataset,data_loader.dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "# Example usage:\n",
    "file_path = 'Datasets/soybean+large/soybean-large.data'\n",
    "label_index = 0\n",
    "categorical_feature_index = None  # The features are a string at the third element\n",
    "numerical_indices = None\n",
    "\n",
    "data_loader = TreeDataLoaderWithCategorical(\n",
    "    file_path= file_path,\n",
    "    label_index= label_index,\n",
    "    numerical_indices= numerical_indices,\n",
    "    categorical_feature_index=categorical_feature_index\n",
    ")\n",
    "\n",
    "# Accessing the processed data\n",
    "print(\"Features:\", data_loader.features, data_loader.features.shape)\n",
    "print(\"Categorical Features:\", data_loader.features_categorical, data_loader.features_categorical.shape)\n",
    "print(\"Numerical Features:\", data_loader.features_numerical, data_loader.features_numerical.shape)\n",
    "print(\"Labels:\", data_loader.labels, data_loader.labels.shape)\n",
    "print(\"True Labels for Points:\", data_loader.true_labels_for_points, data_loader.true_labels_for_points.shape)\n",
    "print(\"Dataset:\\n\", data_loader.dataset,data_loader.dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "# Example usage:\n",
    "file_path = 'Datasets/credit+approval/crx.data'\n",
    "label_index = -1\n",
    "categorical_feature_index = None  \n",
    "numerical_indices = np.array([1, 2, 7, 10, 13,14]) \n",
    "\n",
    "data_loader = TreeDataLoaderWithCategorical(\n",
    "    file_path= file_path,\n",
    "    label_index= label_index,\n",
    "    numerical_indices= numerical_indices,\n",
    "    categorical_feature_index=categorical_feature_index\n",
    ")\n",
    "\n",
    "# Accessing the processed data\n",
    "print(\"Features:\", data_loader.features, data_loader.features.shape)\n",
    "print(\"Categorical Features:\", data_loader.features_categorical, data_loader.features_categorical.shape)\n",
    "print(\"Numerical Features:\", data_loader.features_numerical, data_loader.features_numerical.shape)\n",
    "print(\"Labels:\", data_loader.labels, data_loader.labels.shape)\n",
    "print(\"True Labels for Points:\", data_loader.true_labels_for_points, data_loader.true_labels_for_points.shape)\n",
    "print(\"Dataset:\\n\", data_loader.dataset,data_loader.dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full Data Loader and Objective Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min Height Objective - Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "# Promoter data set\n",
    "file_path = 'Datasets/molecular+biology+promoter+gene+sequences/promoters.data'\n",
    "label_index = 0\n",
    "categorical_feature_index = 2  \n",
    "numerical_indices = None\n",
    "\n",
    "data_loader = TreeDataLoaderWithCategorical(\n",
    "    file_path= file_path,\n",
    "    label_index= label_index,\n",
    "    numerical_indices= numerical_indices,\n",
    "    categorical_feature_index=categorical_feature_index\n",
    ")\n",
    "\n",
    "print(\"Features:\", data_loader.features, data_loader.features.shape)\n",
    "print(\"Categorical Features:\", data_loader.features_categorical, data_loader.features_categorical.shape)\n",
    "print(\"Numerical Features:\", data_loader.features_numerical, data_loader.features_numerical.shape)\n",
    "print(\"Labels:\", data_loader.labels, data_loader.labels.shape)\n",
    "print(\"True Labels for Points:\", data_loader.true_labels_for_points, data_loader.true_labels_for_points.shape)\n",
    "print(\"Dataset:\\n\", data_loader.dataset,data_loader.dataset.shape)\n",
    "\n",
    "\n",
    "features = data_loader.features\n",
    "features_categorical = data_loader.features_categorical\n",
    "features_numerical = data_loader.features_numerical\n",
    "labels = data_loader.labels\n",
    "true_labels_for_points = data_loader.true_labels_for_points\n",
    "dataset = data_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_problems.min_height_tree_categorical_module import find_min_depth_tree_categorical\n",
    "min_depth_tree, min_depth_literals, min_depth,solution = find_min_depth_tree_categorical(features, features_categorical, features_numerical, labels, true_labels_for_points, dataset)\n",
    "print(\"Minimum Depth Tree Structure: \", min_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max Accuracy Objective - Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_problems.fixed_height_tree_categorical_module import find_fixed_depth_tree_categorical\n",
    "tree_with_thresholds, literals, depth, solution, cost = find_fixed_depth_tree_categorical(features, features_categorical, features_numerical, labels, true_labels_for_points, dataset, 2)\n",
    "print(\"The cost of the solution is: \", cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "file_path_to_test = 'Datasets/wine/wine.data'\n",
    "delimiter = ','\n",
    "label_position = 0 \n",
    "\n",
    "data_loader = TreeDataLoaderBinaryNumerical(file_path=file_path_to_test, delimiter=delimiter, label_position= label_position)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Features:\", data_loader.features, data_loader.features.shape)\n",
    "print(\"Labels:\", data_loader.labels, data_loader.labels.shape)\n",
    "print(\"True Labels for Points:\", data_loader.true_labels_for_points, data_loader.true_labels_for_points.shape)\n",
    "print(\"Dataset:\\n\", data_loader.dataset,data_loader.dataset.shape)\n",
    "\n",
    "\n",
    "features = data_loader.features\n",
    "labels = data_loader.labels\n",
    "true_labels_for_points = data_loader.true_labels_for_points\n",
    "dataset = data_loader.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min Height Objective - Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_problems.min_height_tree_module import *\n",
    "min_depth_tree, min_depth_literals, min_depth,solution = find_min_depth_tree(features, labels, true_labels_for_points, dataset)\n",
    "print(f\"Found at depth: {min_depth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_problems.fixed_height_tree_module import *\n",
    "depth = 2 \n",
    "tree_with_thresholds, literals, depth, solution, cost = find_fixed_depth_tree(features, labels, true_labels_for_points, dataset,depth)\n",
    "print(\"The cost of the solution is: \", cost)\n",
    "depth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
