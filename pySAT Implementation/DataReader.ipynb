{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Banknote dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankfile_path = 'Datasets/data_banknote_authentication.txt'\n",
    "\n",
    "# Initialize lists to hold features and labels\n",
    "features_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Open and read the file\n",
    "with open(bankfile_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line into components and convert them to the appropriate types\n",
    "        components = line.strip().split(',')\n",
    "        features = [float(num) for num in components[:-1]]  # Convert feature values to floats\n",
    "        label = int(components[-1])  # Convert the label to an integer\n",
    "        features_list.append(features)\n",
    "        labels_list.append(label)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "dataset = np.array(features_list)\n",
    "true_labels_for_points = np.array(labels_list)\n",
    "labels = np.unique(true_labels_for_points)\n",
    "\n",
    "# Create a features array with numerical labels as strings\n",
    "features = np.array([str(i) for i in range(dataset.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(features))\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(labels))\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Breast Cancer path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_path = 'Datasets/breast+cancer+coimbra/dataR2.csv'\n",
    "df = pd.read_csv(cancer_path)\n",
    "\n",
    "# Now, let's convert the DataFrame into the required numpy arrays\n",
    "# Extracting feature values and labels\n",
    "features_list = df.iloc[:, :-1].values\n",
    "\n",
    "# Extract the last column for labels\n",
    "labels_list = df.iloc[:, -1].values\n",
    "\n",
    "\n",
    "# Converting lists to NumPy arrays\n",
    "dataset = np.array(features_list)\n",
    "true_labels_for_points = np.array(labels_list)\n",
    "\n",
    "# Create a features array with numerical labels as strings\n",
    "features = np.array([str(i) for i in range(dataset.shape[1])])\n",
    "\n",
    "# Convert unique labels to numpy array\n",
    "labels = np.unique(true_labels_for_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cryotherapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cryo_path = 'Datasets/Cryotherapy.xlsx'\n",
    "df = pd.read_excel(cryo_path)\n",
    "\n",
    "# Now, let's convert the DataFrame into the required numpy arrays\n",
    "# Extracting feature values and labels\n",
    "features_list = df.iloc[:, :-1].values\n",
    "\n",
    "# Extract the last column for labels\n",
    "labels_list = df.iloc[:, -1].values\n",
    "\n",
    "\n",
    "# Converting lists to NumPy arrays\n",
    "dataset = np.array(features_list)\n",
    "true_labels_for_points = np.array(labels_list)\n",
    "\n",
    "# Create a features array with numerical labels as strings\n",
    "features = np.array([str(i) for i in range(dataset.shape[1])])\n",
    "\n",
    "# Convert unique labels to numpy array\n",
    "labels = np.unique(true_labels_for_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Immunotherapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_path = 'Datasets/Immunotherapy.xlsx'\n",
    "df = pd.read_excel(im_path)\n",
    "\n",
    "# Now, let's convert the DataFrame into the required numpy arrays\n",
    "# Extracting feature values and labels\n",
    "features_list = df.iloc[:, :-1].values\n",
    "\n",
    "# Extract the last column for labels\n",
    "labels_list = df.iloc[:, -1].values\n",
    "\n",
    "\n",
    "# Converting lists to NumPy arrays\n",
    "dataset = np.array(features_list)\n",
    "true_labels_for_points = np.array(labels_list)\n",
    "\n",
    "# Create a features array with numerical labels as strings\n",
    "features = np.array([str(i) for i in range(dataset.shape[1])])\n",
    "\n",
    "# Convert unique labels to numpy array\n",
    "labels = np.unique(true_labels_for_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ionosphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ion_path = 'Datasets/ionosphere/ionosphere.data'\n",
    "\n",
    "# Initialize lists to hold features and labels\n",
    "features_list = []\n",
    "raw_labels_list = []\n",
    "\n",
    "# Open and read the file\n",
    "with open(ion_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line into components and convert them to the appropriate types\n",
    "        components = line.strip().split(',')\n",
    "        features = [float(num) for num in components[:-1]]  # Convert feature values to floats\n",
    "        raw_labels_list.append(components[-1])  # Keep the original label\n",
    "        features_list.append(features)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "dataset = np.array(features_list)\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit label encoder and return encoded labels\n",
    "true_labels_for_points = label_encoder.fit_transform(raw_labels_list)\n",
    "\n",
    "# Extract the unique labels as sorted array\n",
    "labels = np.sort(np.unique(true_labels_for_points))\n",
    "\n",
    "# Create a features array with numerical labels as strings\n",
    "features = np.array([str(i) for i in range(dataset.shape[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_path = 'Datasets/iris/iris.data'\n",
    "\n",
    "# Initialize lists to hold features and labels\n",
    "features_list = []\n",
    "raw_labels_list = []\n",
    "\n",
    "# Open and read the file\n",
    "with open(iris_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line into components and convert them to the appropriate types\n",
    "        components = line.strip().split(',')\n",
    "        features = [float(num) for num in components[:-1]]  # Convert feature values to floats\n",
    "        raw_labels_list.append(components[-1])  # Keep the original label\n",
    "        features_list.append(features)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "dataset = np.array(features_list)\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit label encoder and return encoded labels\n",
    "true_labels_for_points = label_encoder.fit_transform(raw_labels_list)\n",
    "\n",
    "# Extract the unique labels as sorted array\n",
    "labels = np.sort(np.unique(true_labels_for_points))\n",
    "\n",
    "# Create a features array with numerical labels as strings\n",
    "features = np.array([str(i) for i in range(dataset.shape[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/harisrasul/Desktop/ESC499 THESIS/ESC499-Thesis-SAT-Trees/pySAT Implementation/DataReader.ipynb Cell 18\u001b[0m line \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harisrasul/Desktop/ESC499%20THESIS/ESC499-Thesis-SAT-Trees/pySAT%20Implementation/DataReader.ipynb#X23sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m dataset \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(features_list)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harisrasul/Desktop/ESC499%20THESIS/ESC499-Thesis-SAT-Trees/pySAT%20Implementation/DataReader.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m true_labels_for_points \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(labels_list)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/harisrasul/Desktop/ESC499%20THESIS/ESC499-Thesis-SAT-Trees/pySAT%20Implementation/DataReader.ipynb#X23sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m true_labels_for_points \u001b[39m=\u001b[39m label_encoder\u001b[39m.\u001b[39mfit_transform(true_labels_for_points)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harisrasul/Desktop/ESC499%20THESIS/ESC499-Thesis-SAT-Trees/pySAT%20Implementation/DataReader.ipynb#X23sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Extract the unique labels as sorted array\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/harisrasul/Desktop/ESC499%20THESIS/ESC499-Thesis-SAT-Trees/pySAT%20Implementation/DataReader.ipynb#X23sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msort(np\u001b[39m.\u001b[39munique(true_labels_for_points))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'label_encoder' is not defined"
     ]
    }
   ],
   "source": [
    "user_path = 'Datasets/Data_User_Modeling_Dataset_Hamdi_Tolga_KAHRAMAN.xls'\n",
    "df = pd.read_excel(user_path)\n",
    "\n",
    "# Now, let's convert the DataFrame into the required numpy arrays\n",
    "# Extracting feature values and labels\n",
    "features_list = df.iloc[:, :-1].values\n",
    "\n",
    "# Extract the last column for labels\n",
    "labels_list = df.iloc[:, -1].values\n",
    "\n",
    "\n",
    "# Converting lists to NumPy arrays\n",
    "dataset = np.array(features_list)\n",
    "true_labels_for_points = np.array(labels_list)\n",
    "true_labels_for_points = label_encoder.fit_transform(true_labels_for_points)\n",
    "\n",
    "# Extract the unique labels as sorted array\n",
    "labels = np.sort(np.unique(true_labels_for_points))\n",
    "\n",
    "# Create a features array with numerical labels as strings\n",
    "features = np.array([str(i) for i in range(dataset.shape[1])])\n",
    "\n",
    "# Convert unique labels to numpy array\n",
    "labels = np.unique(true_labels_for_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vertebratla column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verba_path = 'Datasets/vertebral+column/verbex.data'\n",
    "\n",
    "# Initialize lists to hold features and labels\n",
    "features_list = []\n",
    "raw_labels_list = []\n",
    "\n",
    "# Open and read the file\n",
    "with open(verba_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line into components and convert them to the appropriate types\n",
    "        components = line.strip().split(',')\n",
    "        features = [float(num) for num in components[:-1]]  # Convert feature values to floats\n",
    "        raw_labels_list.append(components[-1])  # Keep the original label\n",
    "        features_list.append(features)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "dataset = np.array(features_list)\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit label encoder and return encoded labels\n",
    "true_labels_for_points = label_encoder.fit_transform(raw_labels_list)\n",
    "\n",
    "# Extract the unique labels as sorted array\n",
    "labels = np.sort(np.unique(true_labels_for_points))\n",
    "\n",
    "# Create a features array with numerical labels as strings\n",
    "features = np.array([str(i) for i in range(dataset.shape[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_path = 'Datasets/wine/wine.data'\n",
    "\n",
    "# THIS ONE IS A BIT DIFFERENT LABEL IS ON FIRST FEATURW \n",
    "# Initialize lists to hold features and raw labels\n",
    "features_list = []\n",
    "raw_labels_list = []\n",
    "\n",
    "# Open and read the file\n",
    "with open(wine_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line into components and convert them to the appropriate types\n",
    "        components = line.strip().split(',')\n",
    "        raw_labels_list.append(components[0])  # The first element is the label\n",
    "        features = [float(num) for num in components[1:]]  # The rest are features\n",
    "        features_list.append(features)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "dataset = np.array(features_list)\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit label encoder and return encoded labels\n",
    "true_labels_for_points = label_encoder.fit_transform(raw_labels_list)\n",
    "\n",
    "# Extract the unique labels as sorted array\n",
    "labels = np.sort(np.unique(true_labels_for_points))\n",
    "\n",
    "# Create a features array with numerical labels as strings\n",
    "features = np.array([str(i) for i in range(dataset.shape[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monk-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming 'wine_path' is the path to the text file containing the dataset\n",
    "monk_path = 'Datasets/monk+s+problems/monks-2.train' # Update this to the path of your data file\n",
    "\n",
    "# Initialize lists to hold features and labels\n",
    "features_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Open and read the file\n",
    "with open(monk_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line into components based on whitespace\n",
    "        components = line.strip().split()\n",
    "        # The second-to-last element is the label, and the rest (excluding the last element) are features\n",
    "        labels_list.append(components[0])  # Second-to-last element as label\n",
    "        features = [float(num) for num in components[1:-1]]  # Exclude last two elements\n",
    "        features_list.append(features)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "dataset = np.array(features_list)\n",
    "true_labels_for_points = LabelEncoder().fit_transform(labels_list)\n",
    "\n",
    "# The unique labels\n",
    "labels = np.unique(true_labels_for_points)\n",
    "\n",
    "# Create a features array with numerical labels as strings\n",
    "features = np.array([str(i) for i in range(dataset.shape[1])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree Building now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min height Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n",
      "13\n",
      "['0' '1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12']\n",
      "[0 1 2]\n",
      "178\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "file_path_to_test = 'Datasets/wine/wine.data'\n",
    "X = TreeDataLoaderBinaryNumerical(file_path=file_path_to_test, delimiter=',', label_position= -1)\n",
    "print(len(X.dataset))\n",
    "print(len(X.features))\n",
    "print(X.features)\n",
    "print(X.labels)\n",
    "print(len(X.true_labels_for_points))\n",
    "features = X.features\n",
    "labels = X.labels\n",
    "true_labels_for_points = X.true_labels_for_points\n",
    "dataset = X.dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found at depth: 3\n"
     ]
    }
   ],
   "source": [
    "from min_height_tree_module import *\n",
    "min_depth_tree, min_depth_literals, min_depth,solution = find_min_depth_tree(features, labels, true_labels_for_points, dataset)\n",
    "#print(\"Minimum Depth Tree Structure:\")\n",
    "#for node in min_depth_tree:\n",
    "#    print(node)\n",
    "print(f\"Found at depth: {min_depth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAX SAT PROBLEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169\n",
      "6\n",
      "['0' '1' '2' '3' '4' '5']\n",
      "[0 1]\n",
      "169\n",
      "depth:  2\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from fixed_height_tree_module import * \n",
    "file_path_to_test = 'Datasets/monk+s+problems/monks-2.train'\n",
    "X = TreeDataLoaderBinaryNumerical(file_path=file_path_to_test, delimiter=' ', label_position= 0, custom_exclude=[-1])\n",
    "print(len(X.dataset))\n",
    "print(len(X.features))\n",
    "print(X.features)\n",
    "print(X.labels)\n",
    "print(len(X.true_labels_for_points))\n",
    "\n",
    "features = X.features\n",
    "labels = X.labels\n",
    "true_labels_for_points = X.true_labels_for_points\n",
    "dataset = X.dataset\n",
    "depth = 2\n",
    "print('depth: ', depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost of the solution is:  57\n"
     ]
    }
   ],
   "source": [
    "tree_with_thresholds, literals, depth, solution, cost =find_fixed_depth_tree(features, labels, true_labels_for_points, dataset,depth)\n",
    "print(\"The cost of the solution is: \", cost)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
